---
title: 'Practical Machine Learning Project, Part I: Writeup'
author: "Ambrosio Q. Tria"
output:
  html_document:
    highlight: haddock
    keep_md: yes
    theme: spacelab
    toc: yes
  pdf_document:
    keep_tex: yes
    toc: yes

---
***

```{r setoptions, echo=FALSE}
## global configs
library(knitr)
opts_chunk$set(fig.height=3, fig.width=3, echo=FALSE, cache = TRUE, warning=FALSE, message=FALSE)
```

```{r multiCore}
## enable multi-core processing
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

<a id="pre"></a>

## Preface

```{r}
## load caret library, and other required libraries
library(lattice); library(ggplot2); library(caret); library(randomForest)
library(grid); library(survival); library(Hmisc); library(gridExtra)

## set the seed for the project
set.seed(123456)

```

This is the project for the Coursera/ Johns Hopkins Bloomberg School of Public Health course, Practical Machine Learning. It takes source data sets and creates a machine learning algorithm to predict categorical outcomes on a test data set.

Note that the R code for parallel processing has been enabled for this project.  The Windows 8.1 machine on which the model was built has a hyper-threaded i7 quad core CPU running boosted at about 3.5GHz, effectively providing 8 logical cores to R Studio.  While enabling parallel processing has significantly reduced training time, it is important to understand that untuned models can still take many hours to complete.  

All code included in this writeup can be viewed in the Rmd file from my [Github link](https://github.com/AmbroseT/pml/blob/gh-pages/PML_Project.Rmd).

This web page statistics^[Using [Evernote](http://www.evernote.com), number of words: 2387, number of characters: 17,868]

<a id="eo"></a>

## Executive Summary

This project uses data from accelerometers on the belt, forearm, arm and dumbell of 6 participants.  The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The collected data is split into 2 data set sources; one for training and one for testing. The training data source was used to create a machine learning algorithm that was subsequently applied to the testing data source, predicting the manner in which the participants did the exercise.

The data for this project come from this source:

http://groupware.les.inf.puc-rio.br/har

Supporting figures for this analysis can be found in the [Appendix](#appendix) section of this report. For convenience throughout the main body of the report, links have been created for easier navigation to the supporting figures in the appendix.


<a id="dpe"></a>

## The Data

Before we can apply a machine algorithm to the data, we download the data, clean and prepare it, then explore it to make it tidy and usable for analysis and modeling calculations.

### Preparation

The data sets for the project, once downloaded, will need to be cleaned up for analysis. Once they have been cleaned, the data sets will be explored and analyzed before processing a machine learning algorithm. 

```{r}
## download the training data set
if(!file.exists("training.csv")) {
  message("downloading file. this may take a minute or so...")
  fileUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl, destfile="training.csv")
  datedownloaded <- date()
  
  } else datedownloaded <- "localCopy"
  
## download the testing data set
if(!file.exists("testing.csv")) {
  message("downloading file. this may take a minute or so...")
  fileUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(fileUrl, destfile="testing.csv")
  datedownloaded <- date()
  
  } else datedownloaded2 <- "localCopy"

## read in the data sets, unify NA values
trainData <- read.csv('training.csv', na.strings=c("NA", "", "#DIV/0!"))
testData <- read.csv('testing.csv', na.strings=c("NA", "", "#DIV/0!"))

## =========================================================================
## clean and trim data
## =========================================================================  !!

## remove irrelevant variables 1 - 7
tempNames <- names(trainData[8:160])
newTraining <- trainData[,(names(trainData) %in% tempNames)]

```

The data downloaded come from two sources over the Internet.  

The training data source:

* https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data source:

* https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The training data source is the data set that will be split into 2 more data sets, one for building the model, and the other to test the accuracy and evaluate the model.

The test data source is the data set used to predict the outcomes with the model built using the training data set, which will be submited to the Part II of this project.

Both data sources were saved, respectively, as **```training.csv```** and **```testing.csv```**. Once saved, they were read with paramaters to convert and unify NA type observations, which are NA values, blank observations, and error values such as ```#DIV/0!```. Finally, irrelevant variables were removed from the data set. The irrelevant variables -- [Figure 1](#fig1) -- are the first 7 variables in the training data set, and have no bearing or significance to the data; in fact, they can skew the modeling and prediction if left in.

```{r}
## =========================================================================  
## if there are any variables that consist of more than 90% NA values,
## then remove them

bad.vars <- NULL
na.loop <- dim(newTraining)[2]
for (i in 1:na.loop) {
  if (sum(is.na(newTraining[i])) / dim(newTraining)[1] > .9 ) bad.vars <- c(bad.vars, names(newTraining[i])) 
}

## drop these variables
newTraining2 <- newTraining[,!(names(newTraining) %in% bad.vars)]

## =========================================================================  !!
## convert all variables to num except classe, training and testing
loop <- dim(newTraining2)[2]
for (i in 1:(loop-1)) newTraining2[,i] <- as.numeric(newTraining2[,i])

## =========================================================================  !!
## resulting variables will be used as the selected variable set
## use this later
selected.vars <- names(newTraining2)

## =========================================================================  !!
## splice data into training and test sets
inTrain <- createDataPartition(y=newTraining2$classe, p=0.25, list=FALSE)
training <- newTraining2[inTrain,]
testing <- newTraining2[-inTrain,]

```

Next, variables have been checked to see if they consist of more than **90%** NA values.  If they do contain more than 90% NA values, then they are removed from the data set. You can look at the names of these variables that have been removed in [Figure 2](#fig2).

Once done, all variables except ```classe``` were converted to numeric class. This is done so that the pre-processing and training functions do not fail with errors and warnings.

The names of the remaining variables in the data set were saved into a variable to be used for analysis, and later for part II. You can look at these selected predictors in [Figure 3](#fig3). There are **`r length(selected.vars)`** predictors.

Now we take the training data source and split it.  It is important to note, in order to assist with reducing training processing time, the training data set (not to be confused with the training data **source**) was given only 25% of the training data source (`r dim(training)[1]` observations and `r dim(training)[2]` variables), leaving 75% for the testing data set (`r dim(testing)[1]` observations and `r dim(testing)[2]` variables).  Rule of thumb for medium to large data set source, you want to split it into 60% for the training, and 40% for the testing. 



<a id="mla"></a>

### Exploratory Data Analysis

Keep in mind, the selected predictors used for the model produced a very accurate result and were used as-is. this choice was made in lieu of the time constraints in the scope of this project. However, it should be noted that more refinement can be performed to reduce the selected predictors even further, such as in the following use of ```featurePlot```, increasing accuracy even more.

We can examine the selected predictors by looking at some sample ```featurePlot``` plots. [Figure 4](#fig4) shows 4 plots created from the first 20 predictors, presented in a 2 x 2 matrix, each plot showing a pairwise plot. These 20 predictors are divided consecutively into these 4 plots. So for example, the top left plot shows a pairwise plot of predictors 1 through 5, ```selected.vars[1:5]```, diagonally labeled from the bottom left to the top right. Looking at all 4 plots, we can see that there are some predictors that are correlated with each other.  

From what we gathered looking at the sample ```featurePlot``` plots, we can calculate what are the highest correlated pairs. We will use a threshold of higher than 0.8 correlation for selection. You can see the results in [Figure 5](#fig5). From this list, 6 variables in particular were identified, paired as:

```
gyros_arm_y       19  18
gyros_arm_x       18  19

magnet_arm_x      24  21
accel_arm_x       21  24

magnet_arm_z      26  25
magnet_arm_y      25  26

```
We take a closer look at the above highly correlated predictors with FeaturePlot, see [Figure 6](#fig6). So, with this information, we can further reduce the selected predictors by weighting a combination of these pairs, creating fewer predictors. However, as mentioned, the selected 53 predictors were used as-is.

```{r}
## some sample featurePlots: first 20 variables divided in 4 consecutive segments
p1 <- featurePlot(x = training[, c(selected.vars[1:5])], y = training$classe, plot="pairs")
p2 <- featurePlot(x = training[, c(selected.vars[6:10])], y = training$classe, plot="pairs")
p3 <- featurePlot(x = training[, c(selected.vars[11:15])], y = training$classe, plot="pairs")
p4 <- featurePlot(x = training[, c(selected.vars[16:20])], y = training$classe, plot="pairs")

## note high correlation plot matching calculation
p5 <- featurePlot(x = training[, c(selected.vars[18:19])], y = training$classe, plot="pairs")
p6 <- featurePlot(x = training[, c(selected.vars[21:24])], y = training$classe, plot="pairs")
p7 <- featurePlot(x = training[, c(selected.vars[25:26])], y = training$classe, plot="pairs")

```


## Machine Learning Algorithm

Now that the data sets have been cleaned, a machine learning algorithm was created. Once created, it will be used to make predictions, where the predictions will be assessed for accuracy.

### Pre-Processing

Setting ```trainControl``` parameters also help with reducing time; specifically setting the ```method``` to ```cv``` (cross validation) and setting the ```number``` of subsamples to a smaller number (used 4 for this project). 

```{r}
## tune the training with trControl / trainControl, selecting
## cross validation, and number of subsamples to take
control <- trainControl(method="cv", number=4)
```

It is important to note that an attempt at Principal Component Analysis (PCA) was previously done to reduce the number of variables. PCA created 41 variables; however, it also produced a less accurate model (around 77% accuracy). Needless to say, that pre-processing was discarded and not included in this report.

### Modeling

Being that the outcome is a categorical variable, we will use Random Forests as the method for building the model. The ```train``` function uses the ```trControl``` attribute with the parameters defined in pre-processing. Random Forests with cross validation evaluates multiple model choices and selects the best fit:

```{r modeling}
## create model using random forests without pre-processing,
## the outcome is categorical, therefore random forests is used
modelFit <- train(classe ~ ., method="rf", trControl = control, data = training, prox = TRUE)
```

Inspecting the best fit model shows a **97.1%** accuracy:

```{r}
##  97.1% accuracy, mtry 27
modelFit  
```

We determine the estimated out of sample error rate by examining the final model selected during the model build; in this case, it is estimated at **2.18%**:

```{r}
## out of sample estimated error rate
modelFit$finalModel
```

We evaluate the model against the testing data set and predict for accuracy. The ```confusionMatrix``` validates that not only is the prediction accurate, it is more accurate than than the accuracy of the model built using the training data set, at **98%** accuracy:

```{r}
## fit model to testing for accuracy, this also identifies the error rates
confusionMatrix(testing$classe, predict(modelFit, testing[,-53]))

## 97.9% accuracy
```


<a id="conc"></a>

## Conclusion

The final submission for part II is based on preparing the testing data source in the same way that the testing data source was prepared, which is to apply the selected predictors and dropping the irrelevant variables.

With 97.1% Random Forest Model accuracy, 98% evaluation accuracy, and 2.18% OOB estimated error rate, the prediction for the training data source was calculated. The testing data source R code is provided below:

```
## 1. remove irrelevant variables 1 - 7 using calculation
pmlTest <- testData[,(names(testData) %in% tempNames)]

## 2. subset with selected.vars from training
pmlTest2 <- pmlTest[,(names(pmlTest) %in% selected.vars)]

## 3. convert all variables to num
loop3 <- dim(pmlTest2)[2]
for (i in 1:(loop3)) {pmlTest2[,i] <- as.numeric(pmlTest2[,i])}

## prediction for PML testing
pred <- predict(modelFit, pmlTest2)

```
This produced 100% correct predictions in Part II, so this validates that using the 53 predictors as-is in this excercise was good enough.

***

<a id="appendix"></a>

##Appendix

All supporting figures can be found in this appendix. For convenience, links have been created for easier navigation back to the main body of the report. Note that captions for the figures have been left out because of the linked figure titles.

[Preface](#pre) |[Executive Summary](#eo) | [Data Preparation and Exploration](#dpe) | [Machine Learning Algorithm](#mla) | [Conclusion](#conc)

<a id="fig1"></a>

### Figure 1: List of Irrelevant Variables

```{r}
##
names(trainData[1:7])
```

[Preface](#pre) |[Executive Summary](#eo) | [Data Preparation and Exploration](#dpe) | [Machine Learning Algorithm](#mla) | [Conclusion](#conc)

<a id="fig2"></a>

### Figure 2: Variables that have been dropped from the training data set 

```{r}
## list of removed variable names
bad.vars
```

[Preface](#pre) |[Executive Summary](#eo) | [Data Preparation and Exploration](#dpe) | [Machine Learning Algorithm](#mla) | [Conclusion](#conc)

<a id="fig3"></a>

### Figure 3: Selected Predictors

```{r}
## sselected variables
selected.vars
```

[Preface](#pre) |[Executive Summary](#eo) | [Data Preparation and Exploration](#dpe) | [Machine Learning Algorithm](#mla) | [Conclusion](#conc)

<a id="fig4"></a>

### Figure 4: Sampling of the Training Data Set with featurePlot

```{r fig.height=10, fig.width=10}
## some sample featurePlots of the training data set
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

[Preface](#pre) |[Executive Summary](#eo) | [Data Preparation and Exploration](#dpe) | [Machine Learning Algorithm](#mla) | [Conclusion](#conc)

<a id="fig5"></a>

### Figure 5: Variables with Highest Correlation

```{r}
## identify highly correlated variables, and plot them
M <- abs(cor(training[, -53]))
diag(M) <- 0
which(M > 0.8, arr.ind=T)
```

[Preface](#pre) |[Executive Summary](#eo) | [Data Preparation and Exploration](#dpe) | [Machine Learning Algorithm](#mla) | [Conclusion](#conc)

<a id="fig6"></a>

### Figure 6: featurePlot of Variables with Highest Correlation

```{r fig.width=11, fig.height=5}
## note high correlation plot matching calculation
grid.arrange(p5, p6, p7, nrow = 1)
```

[Preface](#pre) |[Executive Summary](#eo) | [Data Preparation and Exploration](#dpe) | [Machine Learning Algorithm](#mla) | [Conclusion](#conc)
